# Offline_Wikipedia

ðŸ”— Scraping and downloading the Wikipedia database.

***
### Scraping

I tested a scraper and I found that 3000 web page URL links can be scraped in 1 hour using a multiprocessing Python scraper. This is roughly 72000 links per day and 2.2 million links every month or 31 days. This would take an estimated 27 months or 2.25 years to scrape 59660204 Wikipedia pages. As of 2023, Wikipedia has 59660204 pages.

Also, after an hour long scraper test, file and folder sizes indicate that scraping Wikipedia to .html will require multiple terabytes of storage space. 

#
### Database Download

At roughly 100 GB in size, Wikipedia can be downloaded using a BitTorrent client. Kiwix can open the large .zim format file. Kiwix also has a searchable archive link index for downloading.

[Database Torrent Link](https://download.kiwix.org/zim/wikipedia/wikipedia_en_all_maxi_2023-11.zim.torrent)

![Torrent](https://github.com/sourceduty/Offline_Wikipedia/assets/123030236/96c4f931-7185-43d9-a76a-cca0146932a3)

#
### Off-grid Computer

Off-grid computers use database files to provide access to information without the internet.
 
***
